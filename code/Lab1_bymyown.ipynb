{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/connectome/ahhyun724/.conda/envs/3D_CNN_woPip/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "# dataset and dataloader\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform = transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform = transform)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "           )\n",
      "data shape:  torch.Size([60000, 28, 28])\n",
      "target shape:  torch.Size([60000])\n",
      "tensor([5, 0, 4, 1, 9, 2, 1, 3, 1, 4])\n",
      "torch.Size([28, 28])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,  51, 159, 253, 159,  50,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "          48, 238, 252, 252, 252, 237,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  54,\n",
      "         227, 253, 252, 239, 233, 252,  57,   6,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  10,  60, 224,\n",
      "         252, 253, 252, 202,  84, 252, 253, 122,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 163, 252, 252,\n",
      "         252, 253, 252, 252,  96, 189, 253, 167,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,  51, 238, 253, 253,\n",
      "         190, 114, 253, 228,  47,  79, 255, 168,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,  48, 238, 252, 252, 179,\n",
      "          12,  75, 121,  21,   0,   0, 253, 243,  50,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,  38, 165, 253, 233, 208,  84,\n",
      "           0,   0,   0,   0,   0,   0, 253, 252, 165,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   7, 178, 252, 240,  71,  19,  28,\n",
      "           0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  57, 252, 252,  63,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 253, 252, 195,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0, 198, 253, 190,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 255, 253, 196,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  76, 246, 252, 112,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0, 253, 252, 148,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  85, 252, 230,  25,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   7, 135, 253, 186,  12,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  85, 252, 223,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   7, 131, 252, 225,  71,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  85, 252, 145,   0,   0,   0,   0,   0,\n",
      "           0,   0,  48, 165, 252, 173,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  86, 253, 225,   0,   0,   0,   0,   0,\n",
      "           0, 114, 238, 253, 162,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  85, 252, 249, 146,  48,  29,  85, 178,\n",
      "         225, 253, 223, 167,  56,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  85, 252, 252, 252, 229, 215, 252, 252,\n",
      "         252, 196, 130,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,  28, 199, 252, 252, 253, 252, 252, 233,\n",
      "         145,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,  25, 128, 252, 253, 252, 141,  37,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
      "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]],\n",
      "       dtype=torch.uint8)\n",
      "torch.Size([28])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "# 위의 형태로부터 dataset 이 어떻게 생겼는지 확인해야 -> CustomDataset 을 만들 수 있음\n",
    "print(trainloader.dataset)\n",
    "print(\"data shape: \", trainloader.dataset.data.shape)\n",
    "print(\"target shape: \", trainloader.dataset.targets.shape)\n",
    "print(trainloader.dataset.targets[:10])\n",
    "print(trainloader.dataset.data[1].shape)\n",
    "print(trainloader.dataset.data[1])\n",
    "print(trainloader.dataset.data[1][1].shape)\n",
    "print(trainloader.dataset.data[1][1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAELCAYAAAD+9XA2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/NK7nSAAAACXBIWXMAAAsTAAALEwEAmpwYAAAfR0lEQVR4nO3de7zVU/7H8feqlC4qjVQuFSJkupBL/Zo0o1zLLSKRDPJzHz8aM6ahXJIYM24ZoyHSb+IxKBn9xFQaynmEyfxoItFNKqGiUj9avz/27ttaS/u09z5rn73P6fV8PM7D59P67u93nbOX8znf9f3u9TXWWgEAUFE1it0BAED1QEEBAERBQQEAREFBAQBEQUEBAERBQQEARFGtC4oxprUxxhpjahXh2IuMMT0r+7iIg7GDfO3MY6fCBcUYc64xpswYs94YsyodX2GMMTE6WCjGmG+cry3GmI1OPiDHfY01xtwesW890n1y+3hhrP2XCsZO/LGT3ud5xpjF6Z/rRGNMk5j7LwWMncKMHWffj6WLYptcXlehgmKMuV7SfZLultRcUjNJ/ynpPyTVzvCamhU5ZizW2gZbvyQtkdTH+bfxW7crxl8ZacvdPlprnyhSPwqCsVMYxph2kh6RdIFSP9MNkkZXdj8KibFTWMaYbpIOyOvF1tq8viQ1krReUt8dbDdW0sOSXkpv31PSIZJmSFoj6X1Jpzrbz5B0iZMPkvS6k1ulBs+C9OsfkmTSbTUl3SNptaSPJV2Z3r7WDvq4SFLPdNxD0jJJN0paIWlc2AenH20kDZb0f5I2S/pG0mRnnzdI+pektZKelrRrlj/bHpKW5fvelPoXY6egY2eEpP928gPS+9+t2O87Y6e0x0769bUk/VNS+63HyuX9qcgZShdJdSRNymLb8yTdIWk3SWWSJkuaKmlPSVdLGm+MaZvDsXtLOlKpb7qfpBPS/35puq2TpM6Szsphn67mkppIaqXUG5eRtfZPksZLGmVTf2X0cZr7STpR0n7pvg7a2mCMWZP+SyCTPY0xK40xnxhjfm+MqZ/ft1KSGDsq2NhpJ+ld5xgLlfqlc1DO30lpYuyooL93rpM001r7r3y+gYoUlD0krbbWfrf1H4wxs9Id3miM6e5sO8la+4a1doukjpIaSBpprd1srZ0m6UVJ/XM49khr7Rpr7RJJ09P7lFI/yD9Ya5daa7+UdGee39sWSbdYazdZazfmuQ9Jut9auzzdl8lOP2WtbWytfT3D6+ant20h6WeSjpB0bwX6UWoYOzuW79hpoNRfpq61Sv1SrQ4YOzuW19gxxuwr6TJJN+d74IoUlC8k7eHO9Vlru1prG6fb3H0vdeK9JC1Nv8lbLZa0dw7HXuHEG5QaKMm+g/3m43Nr7bd5vtaVqZ/lstausNbOs9ZusdZ+IumXkvpG6E+pYOzsWF5jR6npj4bBvzWU9HWEPpUCxs6O5Tt2/iDpVmtt+AdJ1ipSUGZL2iTptCy2dZc0Xi5pX2OMe+yWkj5Nx+sl1XPamufQp88k7RvsNx/hEsxen4wxYZ8KvWSzVfW6xZuxk3n7inpfUgfnePsrNUX0YeTjFAtjJ/P2FXWcpLuNMSuMMVuL0mxjzHnZ7iDvX1LW2jWShksabYw5yxizmzGmhjGmo6Ty5vvLlKqavzTG7GKM6SGpj6QJ6fa5ks40xtRL37J2cQ7dekbSNcaYfYwxu0v6VQ6vLc+7ktoZYzoaY3aVNCxoXylp/0jHkjHmp8aYViZlX0kjld2ccZXA2PFEHTtKzav3Mcb8JH3d7VZJz1lrq8UZCmPHE3vsHKTUHyMdtW2arI+k57PdQYX+6rXWjpL0X0pNyaxMfz2i1J0KszK8ZnO6kycpdVfEaEkDrbXz05v8XqmLiCslPaHU/yDZelTSy0q9Ee9Iei6372j7rLUfKvU/5qtK3eURzkH+WdKh6XncidnsM33f+U8yNHdS6ue3Pv3f/5V0TR5dL1mMnUTUsWOtfV+pu5HGS1ql1LWTK/LrfWli7CRij51V6en2FdbarWcoq3O5nrP1tjcAACqkOs3LAwCKiIICAIiCggIAiIKCAgCIgoICAIgipxUtjTHcElaCrLWlvmQ346Y0rbbWNi12J8rD2ClZ2x07nKEAO698lwgBtjt2KCgAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoclptGIDviCOOSOKrrrrKaxs4cKCXP/nkk0n8wAMPeG3vvPNOAXoHVC7OUAAAUVBQAABRGGuzf35NVXvYTc2aNZO4UaNGWb8unLqoV6+el7dt2zaJr7zySq/tnnvuSeL+/ft7bd9++62Xjxw5MomHDx+edf9CPGCr8nTs2NHLp02blsQNGzbMej9r16718h/96EcV6lee3rbWdi7GgbNVncZOoRx33HFJPH78eK/t2GOP9fIPPvgg1mG3O3Y4QwEAREFBAQBEQUEBAERR8rcNt2zZ0str166dxF27dvXaunXr5uWNGzdO4r59+0br07Jly5L4/vvv99rOOOOMJP7666+9tnfffdfLX3vttWh9QmEcddRRXv7ss896uXttLrweGb7/mzdvTuLwmskxxxyTxOEtxO7rkL3u3bsncfjzfv755yu7OwVz5JFHJvGcOXOK2BPOUAAAkVBQAABRlNyUV3m3ZUq53f4by5YtW7x86NChSfzNN994be5te5999pnX9tVXX3l5xFv4UAHhbeGHH354Ej/11FNeW4sWLbLe74IFC7x81KhRSTxhwgSv7Y033khid3xJ0p133pn1MbFNjx49kvjAAw/02qrylFeNGv55wH777ZfErVq18tqMqdxPFHCGAgCIgoICAIiCggIAiKLkrqEsWbLEy7/44gsvj3UNpayszMvXrFmTxD/96U+9tvC2zXHjxkXpA0rDI4884uXhkjn5cq/FSFKDBg2SOLxl3J3vb9++fZTj7+zc1Z5nz55dxJ7EFV7Hu/TSS5M4vOY3f/78SunTVpyhAACioKAAAKKgoAAAoii5ayhffvmllw8ZMsTLe/funcT//Oc/vbZwGRTX3LlzvbxXr15evn79+iRu166d13bttddm7jCqHPcpi5J0yimneHl59+6H1z4mT56cxO6jCyRp+fLlXu6O1/AzST/72c+yOj6yF35eo7oYM2ZMxrbws0+VrXr+xAEAlY6CAgCIouSmvEITJ070cncplnA11w4dOnj5xRdfnMThdIQ7xRV6//33vXzw4MFZ9RWly13S55VXXvHawictuqsGT5kyxWsLbyl2n4gXLpkSTk18/vnnSRyuPO0u7xNOwYW3H4erESMlvN26WbNmRepJYZX30YlwbFc2zlAAAFFQUAAAUVBQAABRlPw1lNC6desytq1duzZjm7s8gSQ9/fTTXh4uUY+q7aCDDvJy9/bzcA569erVXu4+duCJJ57w2sLHFfztb3/bblwRdevW9fLrr7/eywcMGBDlONXNySef7OXhz7Eqc68HucvVhz799NPK6E5GnKEAAKKgoAAAoqCgAACiqHLXUMozbNgwL3eX2HA/LyBJPXv29PKpU6cWrF8ovDp16nh5+Lkjd349/PySu8y5JL311ltJXArz8C1btix2F6qEtm3bZmwLP1tW1bjjOfx8zYcffpjE4diubJyhAACioKAAAKKoVlNe4XIq7q3C4XIVjz76qJdPnz49id0pD0l66KGHvNxdmgOloVOnTl4e3kLqOu2007w8XEEY1c+cOXOK3YUfcJf8OfHEE722888/38uPP/74jPu57bbbkth98mwxcIYCAIiCggIAiIKCAgCIolpdQwktXLgwiQcNGuS1Pf74415+wQUXbDeWpPr163v5k08+mcTuMh0onnvvvdfLw6ceutdJSvGaift0QZYBiq9JkyZ5v9Z9LEY4rsKPH+yzzz5JXLt2ba8tXDLHfc83btzotZWVlXn5pk2bkrhWLf/X9ttvv52x75WNMxQAQBQUFABAFBQUAEAU1foaiuv555/38gULFni5Owd/3HHHeW0jRozw8latWiXxHXfc4bUVe/nonUnv3r2T2H3Er/TDzwq98MILldGlvLnXTcK+z507t5J7UzWF1yHcn+Mf//hHr+2mm27Ker/uo4XDayjfffedl2/YsCGJ582b57U99thjXu5+3i28rrdy5UovX7ZsWRKHywHNnz8/Y98rG2coAIAoKCgAgCh2mimv0Hvvvefl/fr1S+I+ffp4beEtxpdddlkSH3jggV5br169YnURO+Ce+oe3aK5atcrLwyd0FoO7InK4MrZr2rRpXv7rX/+6UF2qVq644govX7x4cRJ37do17/0uWbIkiSdOnOi1/fvf//byN998M+/juAYPHuzlTZs2TeKPP/44yjEKgTMUAEAUFBQAQBQUFABAFDvtNZSQu+zzuHHjvLYxY8Z4ubv0Qffu3b22Hj16JPGMGTOi9Q+5cZeqkIqzRE74FMmhQ4cm8ZAhQ7w297bQ3/3ud17bN998U4DeVX933XVXsbuQt/CjC65nn322EnuSG85QAABRUFAAAFFQUAAAUey011Dc5RQk6ayzzkriI4880msLl4t2hcsrzJw5M0LvUFHFWGolXP4lvE5yzjnnJPGkSZO8tr59+xasX6hewmWkSglnKACAKCgoAIAoqvWUV9u2bZP4qquu8trOPPNML2/evHnW+/3++++TOLwdlaftVR535ddwFdjTTz/dy6+99tqC9OG6665L4t/+9rdeW6NGjbx8/PjxSTxw4MCC9AcoJs5QAABRUFAAAFFQUAAAUVTpayjhdY/+/ft7uXvdpHXr1nkfx32ymuQ/pbHUnwRYnblP5AufchiOjfvvvz+JwyfnffHFF15+zDHHJPEFF1zgtXXo0MHL99lnnyR2lzmXpJdfftnLR48eLSAf7jXCgw46yGuLtWR+DJyhAACioKAAAKIo+SmvZs2aefmhhx6axA8++KDXdvDBB+d9nLKysiS+++67vbbwU83cGlz6atas6eXu0/zCT6WvW7fOy8OncJZn1qxZSTx9+nSv7eabb856P0B53CndGjVK9zygdHsGAKhSKCgAgCgoKACAKEriGkqTJk2S+JFHHvHawhVc999//7yO4c51Sz98Kp57i+fGjRvzOgYq1+zZs5N4zpw5Xlu4YrQrvKU4vE7nCm8pnjBhgpcXakkXIJMuXbp4+dixY4vTke3gDAUAEAUFBQAQBQUFABBFpV1DOfroo5M4fJLdUUcdlcR777133sfYsGGDl7vLbYwYMcJrW79+fd7HQWlYtmxZEoePI7jsssu8fOjQoVnv97777kvihx9+2Gv76KOPcukiEEX4eIZSxRkKACAKCgoAIIpKm/I644wzthvvyLx587z8xRdfTOLvvvvOawtvBV6zZk0OPURVFj45c9iwYeXmQCmbMmWKl5999tlF6kluOEMBAERBQQEAREFBAQBEYcIn3ZW7sTHZb4xKY60t6XsKGTcl621rbedid6I8jJ2Std2xwxkKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoKCgAgCgoKACCKXJevXy1pcSE6gry1KnYHssC4KU2MHeRru2Mnp7W8AADIhCkvAEAUFBQAQBQUFABAFBQUAEAUFBQAQBQUFABAFBQUAEAUFBQAQBQUFABAFBQUAEAUFBQAQBQUFABAFBQUAEAU1bqgGGNaG2OsMSbXZfpjHHuRMaZnZR8XcTB2kK+deexUuKAYY841xpQZY9YbY1al4yuMMSZGBwvFGPON87XFGLPRyQfkuK+xxpjbI/athTHmBWPM8vTAbB1r36WEsVOQsWOMMb8xxiwxxqwzxkwwxjSMtf9SwdgpyNg5xRjzujFmjTFmhTFmjDFmt1z2UaGCYoy5XtJ9ku6W1FxSM0n/Kek/JNXO8JqaFTlmLNbaBlu/JC2R1Mf5t/FbtyvGXxmStkj6H0l9i3DsSsHYKZiBki5Q6ue4l6S6kh4oQj8KhrFTMI0k3a7UuDlE0t5K/YyzZ63N6yt98PWS+u5gu7GSHpb0Unr7nunOzpC0RtL7kk51tp8h6RInHyTpdSe3Sg2eBenXP6RtDwqrKekepZ7y9rGkK9Pb19pBHxdJ6pmOe0haJulGSSskjQv74PSjjaTBkv5P0mZJ30ia7OzzBkn/krRW0tOSds3xZ1wrfZzW+b5PpfjF2Cnc2JH0V0lDnLyrpG8l1Sv2+87YKe2xs53+nSnpf3N5TUXOULpIqiNpUhbbnifpDkm7SSqTNFnSVEl7Srpa0nhjTNscjt1b0pGS2kvqJ+mE9L9fmm7rJKmzpLNy2KeruaQmSj3mcnB5G1pr/yRpvKRRNvVXRh+nuZ+kEyXtl+7roK0N6dPKbnn2r6pj7KigY8cEcR1JB+bwPZQyxo4q7fdOd6UKb9YqUlD2kLTaWvvd1n8wxsxKd3ijMaa7s+0ka+0b1totkjpKaiBppLV2s7V2mqQXJfXP4dgjrbVrrLVLJE1P71NK/SD/YK1daq39UtKdeX5vWyTdYq3dZK3dmOc+JOl+a+3ydF8mO/2Utbaxtfb1Cuy7KmPs7Fi+Y+d/JF2SvjDcSKm/eCWpXgX6UkoYOztW4d87xpheki6UdHMuB65IQflC0h7uXJ+1tqu1tnG6zd33UifeS9LS9Ju81WKl5uuytcKJNyg1UJJ9B/vNx+fW2m/zfK0rUz93doydHct37Dwm6S9KTeG8r9QvPik1nVIdMHZ2rEK/d4wxx0j6b0lnWWs/zOW1FSkosyVtknRaFttaJ14uaV9jjHvslpI+Tcfr5f811TyHPn0mad9gv/mwQe71yRgT9incHuVj7GTevkKstVustbdYa1tba/dRqqh8qm0/o6qOsZN5+wozxnSS9IKkn1tr/57r6/MuKNbaNZKGSxptjDnLGLObMaaGMaajpPrlvLRMqar5S2PMLsaYHpL6SJqQbp8r6UxjTD1jTBtJF+fQrWckXWOM2ccYs7ukX+Xw2vK8K6mdMaajMWZXScOC9pWS9o90LElS+jh10mmddF4tMHY8UceOMaaJMeaA9O3Dh0q6V9KtwV/mVRZjxxN77Bym1JTp1dbayfnso0K3DVtrR0n6L0m/VOqbWynpEaXmbWdleM1mpd7Ik5S6K2K0pIHW2vnpTX6v1J0LKyU9odSFp2w9Kullpd6IdyQ9l9t3tH3p075bJb2q1F0e4RzknyUdmp7HnZjNPtP3nf+knE02KnX3hiTNT+fVBmMnEXvs7KFtdzZNkfRY+gJutcHYScQeO9dLairpz85nY3K6KL/1tjcAACqkWi+9AgCoPBQUAEAUFBQAQBQUFABAFBQUAEAUOa1oaYzhlrASZK0t9SW7GTelabW1tmmxO1Eexk7J2u7Y4QwF2Hnlu0QIsN2xQ0EBAERBQQEAREFBAQBEQUEBAERBQQEAREFBAQBEQUEBAERBQQEAREFBAQBEQUEBAERBQQEAREFBAQBEQUEBAERBQQEAREFBAQBEkdMDtpAydOjQJB4+fLjXVqPGthrdo0cPr+21114raL8AVB277bZbEjdo0MBrO+WUU7y8adNtz7K69957vbZNmzYVoHf54QwFABAFBQUAEAUFBQAQBddQsjBo0CAvv/HGG5N4y5YtGV9nrS1UlwCUuNatW3u5+3tDkrp06ZLEhx12WNb7bdGihZdfc801uXeuQDhDAQBEQUEBAETBlFcWWrVq5eW77rprkXqCynD00Ucn8fnnn++1HXvssV7erl27jPu54YYbvHz58uVJ3K1bN6/tqaeeSuKysrLsO4uiOvjgg738F7/4RRIPGDDAa6tbt66XG2OSeOnSpV7b119/7eWHHHJIEvfr189rGz16dBLPnz8/i14XDmcoAIAoKCgAgCgoKACAKLiGsh09e/b08quvvjrjtuGcZe/evZN45cqVcTuGgjjnnHO8/L777kviPfbYw2tz570lacaMGUnsLo8hSXfffXfGY4b7cV977rnnlt9hVKpGjRol8V133eW1hWPHXU5lRxYsWJDEJ5xwgte2yy67eLn7eyYck2FeTJyhAACioKAAAKKgoAAAouAaSpr7uYDHH3/ca3PnUEPhPPnixYvjdgxR1Kq1bah37tzZa3v00Ue9vF69ekk8c+ZMr+22227z8tdffz2J69Sp47U988wzXn788cdn7N9bb72VsQ3FdcYZZyTxJZdckvd+Fi5c6OW9evVK4vBzKG3atMn7OMXEGQoAIAoKCgAgCqa80i688MIk3muvvcrd1r1V9MknnyxUlxCRu4TKmDFjyt32lVdeSeLwttB169ZlfF24bXlTXMuWLfPyJ554otw+oXjOPvvsrLddtGhREs+ZM8drC1cbDqe5XO5SK1UJZygAgCgoKACAKCgoAIAodtprKOFyBT//+c+TOHwK45o1a7z89ttvL1i/EEd4e+9NN92UxOGTNN3lvyVp6NChSVzeNZPQb37zm6y3DZ+y9/nnn2f9WlSuSy+9NIkHDx7stU2dOtXLP/rooyRetWpV3sds1qxZ3q8tJs5QAABRUFAAAFFQUAAAUew011Bat27t5c8++2zWr33ggQe8fPr06TG6hIhuvvlmL3evmUjS5s2bk/jll1/22sLPB2zcuDHjccLHP7ufNWnZsqXXFi5R7157mzRpUsZjoLS4j24eNmxYpRyzS5culXKc2DhDAQBEQUEBAESx00x5nXjiiV7evn37jNv+/e9/93L3CX4oHY0bN07iK664wmsLbw12p7lOP/30rI8Rrvo6fvx4Lz/iiCMyvvavf/2rl48aNSrr46LqC28Nr1+/ftav/fGPf5yxbdasWV4+e/bs3DpWQJyhAACioKAAAKKgoAAAoqjW11DcufKRI0eWu6375D13KXtJWrt2bdR+IY7atWsncbiUTsidz95zzz29tosuusjLTz311CQ+7LDDvLYGDRp4uXutJrxu89RTT3n5+vXry+0jSp/7NE9JOvTQQ738lltuSeKTTz653H3VqLHt7/lwuaeQe+tyOF6///77cl9bmThDAQBEQUEBAERBQQEARFGtrqFUZHmVjz/+OIlXrlwZq0soIHc5lXD596ZNm3r5J598ksThtY7yuHPX0g+Xs2/RokUSr1692mubPHly1sdB6dhll128vFOnTkkc/k5x33/JX7YnHDvh50Xcz8aF12ZCtWpt+1V95plnem3u5+Tc/yeKgTMUAEAUFBQAQBTVasorXDV2R7fiuXZ0WzFKj/skzXA5lRdffNHLmzRpksQLFy702sKVf8eOHZvEX375pdc2YcIEL3enPMI2VA3u7efSD5dpeu655zK+dvjw4V4+bdq0JH7jjTe8NncMhtuGt6eH3CncO++802tbsmRJEk+cONFr27RpU7n7jY0zFABAFBQUAEAUFBQAQBRV+hpKx44dvdx9et6OhPPmH3zwQYwuoUjKysq8PLxtOF/du3f38mOPPdbL3et07q3nKG3urcHhdZAhQ4ZkfN2UKVO8PHyaq3tdLxyDL730kpe7S9SHt/uGjzpwr7GcdtppXpv7SIVXX33Va7vrrru8/KuvvlImc+fOzdiWLc5QAABRUFAAAFFQUAAAUVTpayhTp0718t133z3jtm+++aaXDxo0qBBdQjVTt25dLw8/2+Qu48LnUEpXzZo1vfy2225L4htuuMFrCx8z8Ktf/SqJw/fYvWYiSZ07d07iBx980Gtzl3CRpAULFiTx5Zdf7rVNnz7dyxs2bJjEXbt29doGDBiQxO6jFyTplVdeUSZLly718v322y/jttniDAUAEAUFBQAQhcll5VVjTPYbV4LwSWXlLbUycOBAL//LX/5SkD4Vg7XWFLsP5Sm1cVMR4Zhz//8JV54NV0AuQW9bazvveLPiiTV2wikl93bfDRs2eG2DBw/2cndq/eijj/bawqcnnnTSSUkcTpfeeuutXv74448ncTj9lK/+/ft7+XnnnZdx2+uuu87LP/roo1wOtd2xwxkKACAKCgoAIAoKCgAgiip3DcWddwxv/S3vGsr+++/v5YsXL47ar2LiGkrhnHDCCV4eLp/BNZTCijV2PvvsMy93l0UJl3ifP3++l9evXz+J27Rpk/Uxhw0b5uXhsvPh9bgqhmsoAIDCoaAAAKIo+U/KhysK9+zZM4nDKa5wxc6HHnooiVeuXBm/c6j2wqlSVE0rVqzwcnfKq06dOl5bhw4dMu4nnPKcOXOml7tPTFy0aJHXVsWnuLLCGQoAIAoKCgAgCgoKACCKkr+G0rhxYy9v3rx5xm0//fRTLw9XEQVy9Y9//MPLa9Tw/wYr71Z1lI7wyZunn356Eh9++OFe26pVq7z8scceS+LwiYfhddudHWcoAIAoKCgAgCgoKACAKEr+GgpQTO+9956Xu0/Zk/zPqRxwwAFeWxVYemWn8fXXX3v5uHHjthujYjhDAQBEQUEBAERR8lNe4cqfs2bNSuJu3bpVdnewkxsxYoSXjxkzJonvuOMOr+3qq6/28nnz5hWuY0AJ4AwFABAFBQUAEAUFBQAQRZV7YiN+iCc2Vp6GDRt6+TPPPJPE7qMVJOm5557z8osuuiiJ169fX4De5WyneWIjouOJjQCAwqGgAACioKAAAKLgGko1wDWU4nGvqYSfQ7n88su9vH379klcIp9J4RoK8sU1FABA4VBQAABRMOVVDTDlhTwx5YV8MeUFACgcCgoAIAoKCgAgilyXr18taXEhOoK8tSp2B7LAuClNjB3ka7tjJ6eL8gAAZMKUFwAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIAoKCgAgCgoKACAKCgoAIIr/B8sw0oM+p7UbAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(trainloader.dataset.data[0].shape) #first image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(6):\n",
    "  plt.subplot(2,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(trainloader.dataset.data[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Ground Truth: {}\".format(trainloader.dataset.targets[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000])\n",
      "60000\n",
      "60000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(trainloader.dataset.data.shape)\n",
    "print(trainloader.dataset.targets.shape)\n",
    "print(len(trainloader.dataset.data))\n",
    "print(len(trainloader.dataset.targets))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "torch.Size([28, 28])\n",
      "torch.Size([])\n",
      "torch.float32\n",
      "torch.uint8\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "trainloader.dataset[0] #tuple\n",
    "trainloader.dataset[0][0] #first image\n",
    "trainloader.dataset[0][1] #first label\n",
    "trainloader.dataset.data[0] #first image\n",
    "trainloader.dataset.targets[0] #first label\n",
    "\n",
    "print(trainloader.dataset[0][0].shape) #first image \"TRANSFORMED\" by dataloader\n",
    "# print(trainloader.dataset[0][1].shape) #first label => int \"TRANSFORMED\" by dataloader\n",
    "print(trainloader.dataset.data[0].shape) #first image \"RAW\"\n",
    "print(trainloader.dataset.targets[0].shape) #first label \"RAW\"\n",
    "\n",
    "print(trainloader.dataset[0][0].dtype) #first image\n",
    "#print(trainloader.dataset[0][1].dtype) #first label => int\n",
    "print(trainloader.dataset.data[0].dtype) #first image\n",
    "print(trainloader.dataset.targets[0].dtype) #first label\n",
    "\n",
    "\n",
    "# first index of the dataset ;  image\n",
    "# [0][0] -> image, first\n",
    "# [0][1] -> label, first\n",
    "# [1][0] -> image, second\n",
    "# [1][1] -> label, second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "# Make CustomDataset to check whether downloading is properly done + collate function\n",
    "    def __init__(self):\n",
    "        x_data = trainloader.dataset.data\n",
    "        y_data = trainloader.dataset.targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "    def __collate_fn__(self, batch):\n",
    "        return torch.stack(batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# below followed copilot\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.targets[idx]\n",
    "    def collate_fn(self, batch):\n",
    "        return torch.stack(batch)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START FROM HERE\n",
    "### 1. Data set and Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from here\n",
    "\n",
    "# import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets,transforms\n",
    "\n",
    "# dataset and dataloader\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "trainset = datasets.MNIST(root='./data', train=True, download=True, transform = transform)\n",
    "testset = datasets.MNIST(root='./data', train=False, download=True, transform = transform)\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "trainloader = DataLoader(trainset, batch_size = BATCH_SIZE, shuffle = True)\n",
    "testloader = DataLoader(testset, batch_size = BATCH_SIZE, shuffle = False) \n",
    "# currently no collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MyNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)\n",
    "        self.fc2 = nn.Linear(128,64)\n",
    "        self.fc3 = nn.Linear(64,32)\n",
    "        self.fc4 = nn.Linear(32,10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        x = x.reshape(-1, 28*28)\n",
    "            # -1 은 마지막 차원을 28*28 으로 만들기 위해 나머지는 알아서 차원 결정해줘~\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "        #[batch_size, 1, 28, 28] -> [batch_size, 28*28]\n",
    "        # 1: gray scale, 28:height, 28:width\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 확인 위한 코드\n",
    "model = MyNN()\n",
    "output = model(trainloader.dataset[3][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# instantiation of MyNN\n",
    "\n",
    "network = MyNN()\n",
    "print(network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Define & Perform Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train function\n",
    "\n",
    "def train(model, optim, loss_fn, train_loader, epochs, device):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = 0.0\n",
    "        model.train() # turn the training mode on\n",
    "        for batch in train_loader:\n",
    "            data, target = batch\n",
    "            data, target = data.to(device), target.to(device) # send to GPU\n",
    "\n",
    "            # Compute gradient\n",
    "            optim.zero_grad() # initialize the gradient\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss /= len(train_loader.dataset) # normalize training loss\n",
    "        print(f\"Epoch: {epoch}, Loss: {train_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 2.521319267814602e-05\n",
      "Epoch: 1, Loss: 6.5718342664611196e-06\n",
      "Epoch: 2, Loss: 4.0878546034338715e-06\n",
      "Epoch: 3, Loss: 2.9352267882399467e-06\n",
      "Epoch: 4, Loss: 2.2981731053944823e-06\n",
      "Epoch: 5, Loss: 1.8761121829099258e-06\n",
      "Epoch: 6, Loss: 1.5963724451788251e-06\n",
      "Epoch: 7, Loss: 1.3880269079851073e-06\n",
      "Epoch: 8, Loss: 1.2108282335685047e-06\n",
      "Epoch: 9, Loss: 1.0566750134082609e-06\n"
     ]
    }
   ],
   "source": [
    "# perform actual training\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "#optimizer = torch.optim.Adam(network.parameters(), lr=0.001)\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.01, momentum=0.9)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "trainloader = trainloader\n",
    "EPOCH = 10\n",
    "network.to(device) # send to GPU ; 이거 안 하면 model 이 서로 다른 device 에 있다고 그럼\n",
    "\n",
    "train(network, optimizer, loss_fn, trainloader, EPOCH, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model,loss_fn, test_loader, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad(): #if not, it will consume memory\n",
    "        for batch in test_loader:\n",
    "            data, target = batch\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += loss_fn(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True) #[batch_size, num_classes], argmax over num_classes\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "            # view as same dimension as target     \n",
    "            # pred.eq(target.view_as(pred)) -> [batch_size, 1] -> True or False   \n",
    "        print(\"loss: \", test_loss)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "    print(f'Test Loss: {test_loss}, Accuracy: {correct}/{len(test_loader.dataset)} {100.*correct/len(test_loader.dataset)}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:  13.976509554544464\n",
      "Test Loss: 0.0013976509554544463, Accuracy: 9742/10000 97.42%\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "test(network, loss_fn, testloader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n",
      "=====parameters=====\n",
      "name:  fc1.weight\n",
      "weights shape:  torch.Size([128, 784])\n",
      "weights:  Parameter containing:\n",
      "tensor([[-0.0330, -0.0054, -0.0342,  ..., -0.0136, -0.0145,  0.0178],\n",
      "        [ 0.0073, -0.0172, -0.0272,  ..., -0.0058, -0.0112,  0.0325],\n",
      "        [-0.0269,  0.0244, -0.0231,  ...,  0.0094, -0.0296, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0106,  0.0116, -0.0206,  ...,  0.0073,  0.0137, -0.0123],\n",
      "        [-0.0319, -0.0137, -0.0115,  ..., -0.0051, -0.0015,  0.0027],\n",
      "        [ 0.0312,  0.0069,  0.0099,  ...,  0.0145,  0.0003,  0.0097]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc1.bias\n",
      "weights shape:  torch.Size([128])\n",
      "weights:  Parameter containing:\n",
      "tensor([ 0.0310,  0.0522, -0.1483, -0.0355,  0.0285, -0.0573,  0.1163, -0.1793,\n",
      "        -0.0059, -0.0237,  0.0454, -0.0147, -0.0291, -0.0776,  0.0111,  0.0205,\n",
      "         0.0038,  0.0384, -0.1121,  0.0760, -0.0033, -0.0899,  0.0175, -0.1910,\n",
      "        -0.0660, -0.0766, -0.0457,  0.0113, -0.0182,  0.0126,  0.0262, -0.1074,\n",
      "         0.0830, -0.0178, -0.0841, -0.2541, -0.0003, -0.1276,  0.0768,  0.0388,\n",
      "        -0.0634, -0.0297,  0.0795, -0.0432,  0.0796, -0.0486, -0.0653, -0.0778,\n",
      "         0.0727,  0.1358,  0.0309, -0.0081,  0.0867, -0.0268,  0.0551,  0.0634,\n",
      "         0.0084, -0.0865, -0.0638, -0.0288,  0.0111, -0.0119, -0.1085, -0.0535,\n",
      "         0.0100, -0.0324, -0.0330,  0.0077,  0.0036,  0.1218, -0.0313,  0.0760,\n",
      "        -0.0604,  0.0187,  0.0645, -0.0822,  0.0281, -0.0024, -0.0223,  0.0427,\n",
      "         0.0343,  0.0152,  0.0660,  0.1659, -0.0192, -0.0621,  0.0325, -0.0219,\n",
      "        -0.0497, -0.0551, -0.0782, -0.1017, -0.1735,  0.1067,  0.0467,  0.0988,\n",
      "        -0.0648, -0.0521,  0.0657, -0.0440, -0.0247, -0.2453, -0.0840,  0.0215,\n",
      "        -0.0927,  0.0499, -0.0648, -0.0034,  0.0307,  0.0051, -0.0284, -0.0082,\n",
      "         0.0222,  0.0268, -0.0370,  0.0368,  0.1135, -0.0571,  0.0202, -0.0336,\n",
      "        -0.0546,  0.0136,  0.0210, -0.1435,  0.0060, -0.0458, -0.0304,  0.0124],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc2.weight\n",
      "weights shape:  torch.Size([64, 128])\n",
      "weights:  Parameter containing:\n",
      "tensor([[ 0.0699, -0.1530,  0.1186,  ..., -0.1666,  0.1542,  0.0646],\n",
      "        [ 0.0938,  0.0511,  0.1039,  ...,  0.0061,  0.0592, -0.0076],\n",
      "        [ 0.0436, -0.0220, -0.0122,  ...,  0.0869,  0.0733, -0.0859],\n",
      "        ...,\n",
      "        [ 0.1300, -0.1648, -0.0049,  ..., -0.2016,  0.2896,  0.0362],\n",
      "        [ 0.0369, -0.0069,  0.2003,  ...,  0.0304,  0.1292,  0.0740],\n",
      "        [-0.0908,  0.0299, -0.1856,  ...,  0.2484, -0.1664, -0.0589]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc2.bias\n",
      "weights shape:  torch.Size([64])\n",
      "weights:  Parameter containing:\n",
      "tensor([-0.0527,  0.0163,  0.0675,  0.1976,  0.0817,  0.1805, -0.0111,  0.1616,\n",
      "         0.0013,  0.0066,  0.1918,  0.1463,  0.0434,  0.0577,  0.0626, -0.0408,\n",
      "        -0.1070,  0.1327,  0.1688,  0.0620,  0.0391, -0.0270,  0.1133, -0.0157,\n",
      "         0.0636, -0.0276,  0.1449, -0.0207,  0.0688,  0.1199, -0.0511,  0.0069,\n",
      "         0.0289, -0.0322, -0.0642,  0.1589,  0.0694, -0.0665,  0.0763, -0.0583,\n",
      "         0.0619, -0.0332, -0.0478, -0.0473, -0.0552, -0.0715, -0.1163, -0.0814,\n",
      "         0.0215, -0.0202, -0.0496, -0.0497,  0.0131,  0.1206, -0.0929, -0.0165,\n",
      "         0.0313, -0.0686, -0.0940,  0.1665,  0.1168,  0.0286,  0.0146,  0.1257],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc3.weight\n",
      "weights shape:  torch.Size([32, 64])\n",
      "weights:  Parameter containing:\n",
      "tensor([[-0.1467,  0.1244,  0.0229,  ...,  0.0044, -0.1939, -0.0456],\n",
      "        [-0.0448,  0.2650, -0.1056,  ...,  0.1370, -0.3713, -0.4108],\n",
      "        [ 0.0201,  0.2276,  0.0368,  ..., -0.0508,  0.0098,  0.2827],\n",
      "        ...,\n",
      "        [-0.2167,  0.0041,  0.1304,  ..., -0.3298, -0.1498,  0.4338],\n",
      "        [ 0.0810, -0.0712,  0.0777,  ...,  0.4210, -0.1426, -0.2544],\n",
      "        [ 0.3099, -0.2245,  0.0949,  ...,  0.1410, -0.1325, -0.0669]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc3.bias\n",
      "weights shape:  torch.Size([32])\n",
      "weights:  Parameter containing:\n",
      "tensor([ 0.0455,  0.1263, -0.0996,  0.0034, -0.0812,  0.1014, -0.0076, -0.0511,\n",
      "         0.0597,  0.1520,  0.0552,  0.0058,  0.0660,  0.1006, -0.1893,  0.0873,\n",
      "        -0.0620, -0.1334, -0.0826,  0.1020, -0.1674, -0.0529,  0.0418,  0.0317,\n",
      "         0.0425, -0.0327, -0.0075, -0.0166,  0.1408,  0.0762,  0.0789,  0.0243],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc4.weight\n",
      "weights shape:  torch.Size([10, 32])\n",
      "weights:  Parameter containing:\n",
      "tensor([[-1.2384, -0.4354,  0.4775,  0.7206,  0.5117, -0.4341, -1.1096,  1.1938,\n",
      "          0.8528, -0.5689, -0.2095, -0.2306, -0.9451, -0.3992,  1.1545,  0.6006,\n",
      "          1.1306, -0.2959, -0.7608, -0.2664,  0.6091, -0.4507, -0.8968, -0.3013,\n",
      "          0.3946, -0.2926,  1.0082, -0.4256, -1.3059, -1.8735, -0.3335,  0.6844],\n",
      "        [ 0.5433,  0.9458, -0.9600, -0.5445, -0.2385,  0.8177,  0.7829, -0.8220,\n",
      "         -1.4756,  0.1976, -0.5400, -0.0385,  0.6289,  1.2883,  0.7369, -0.4984,\n",
      "         -0.4495,  0.5353,  1.1385,  1.2840, -0.9552, -0.2479,  0.7549,  0.4948,\n",
      "          0.5470, -0.0189, -0.8034, -0.6140,  0.6297,  0.3768, -0.7953, -1.0944],\n",
      "        [-0.2680, -0.6150, -1.3875, -1.0004,  0.3384, -0.5444,  0.7721, -0.0249,\n",
      "         -0.0051, -0.5633,  1.3033, -0.3962,  1.2707, -0.4366,  0.6077,  0.7679,\n",
      "          1.0958,  0.1425,  0.6311, -0.5664,  0.1757, -0.0182,  0.1985,  1.5969,\n",
      "          0.1876,  0.4590, -0.6774,  0.3438, -1.2508, -0.0359, -0.1998,  0.1526],\n",
      "        [ 0.7703, -0.4121,  0.8139, -1.0195, -1.6719,  0.3776,  0.6081,  1.1411,\n",
      "          0.7038,  1.4468, -0.7712, -0.4390,  1.0177, -0.2945,  0.6126, -0.1309,\n",
      "          0.2395, -0.8052,  0.2450, -0.6153,  0.1457, -0.0485, -0.9719,  0.0404,\n",
      "          0.5446, -0.5886, -1.1242,  0.0689,  0.7060,  0.7459, -0.2583, -1.4060],\n",
      "        [-0.9140, -0.1516,  0.0819,  0.6649, -0.0290, -0.5508, -0.3060, -0.5399,\n",
      "         -1.0235, -0.5033,  1.0469,  1.0990, -0.4348, -0.3585, -0.8854,  0.9792,\n",
      "         -0.6689,  0.4719, -0.5522,  0.2916, -0.5770,  1.2499,  0.4436, -0.1235,\n",
      "         -0.9518,  1.2009,  0.2821,  0.7638,  0.4353,  0.3236,  0.9292,  0.0040],\n",
      "        [ 0.7496, -0.4218,  0.2369,  1.8763,  0.7240, -0.4662, -0.3501,  0.0685,\n",
      "          0.7655,  1.7404, -0.4250, -0.1466, -1.1302, -0.2439, -1.1763, -0.0080,\n",
      "         -0.7117, -1.5739,  0.0525, -0.0362,  0.6319, -0.0369, -0.6006, -0.1639,\n",
      "          0.4998, -0.3795,  0.5885, -0.6058,  0.4349,  0.6953, -0.1712, -0.1361],\n",
      "        [-0.8299, -0.6230,  0.3710,  1.1125,  0.5041, -0.7144, -1.1124, -0.8003,\n",
      "         -1.3396, -0.2809,  0.8481, -0.8635, -0.9747, -0.4290,  0.8253,  0.7037,\n",
      "         -0.9253,  0.2644,  1.1062, -0.3637,  0.3144, -0.5764,  0.6968, -0.8137,\n",
      "          0.6949,  1.1971,  0.5477, -0.8914, -1.1534,  0.7878, -1.0999,  0.2684],\n",
      "        [ 0.2862,  0.8298, -1.6059, -0.1912, -0.3288,  0.6298,  0.5761,  1.0426,\n",
      "          0.4881, -0.4763,  0.0899,  0.4062,  0.6884,  1.7086, -0.2944, -0.1478,\n",
      "          0.6434,  0.5194, -0.5196,  0.2875, -0.9247,  0.0680, -1.0166,  0.6787,\n",
      "         -0.9938, -0.3417,  0.1003,  0.6820,  0.4853, -1.3419,  0.9767,  0.2505],\n",
      "        [ 0.6295, -0.7480,  0.6904, -1.1241,  1.1576,  0.1538,  1.2679, -0.9861,\n",
      "          0.4637, -1.1760, -0.6277, -0.5850,  0.2219, -0.6575, -0.7506, -1.0607,\n",
      "          0.2717,  0.4695, -0.3953, -0.4371,  0.3812, -0.7179,  1.1707, -0.9587,\n",
      "          0.5123, -0.5874, -0.9263, -0.6857,  0.9653,  0.4691, -0.3992,  1.1457],\n",
      "        [ 0.1616,  1.1009,  0.9357, -0.9305, -0.7437,  1.3158, -1.0392,  0.0086,\n",
      "          0.6851,  0.1216, -0.8501,  0.7945, -0.3714, -0.7694, -0.7447, -1.2795,\n",
      "         -0.8444,  0.4980, -0.7487,  0.0789,  0.2119,  1.1515,  0.1625, -0.4898,\n",
      "         -0.5664, -0.8355,  0.8560,  0.9204,  0.2701,  0.1533,  0.9255,  0.1135]],\n",
      "       device='cuda:0', requires_grad=True)\n",
      "==========\n",
      "name:  fc4.bias\n",
      "weights shape:  torch.Size([10])\n",
      "weights:  Parameter containing:\n",
      "tensor([ 0.2286, -0.0657, -0.2690, -0.0322, -0.2262,  0.1610, -0.2609, -0.0872,\n",
      "        -0.1328,  0.0542], device='cuda:0', requires_grad=True)\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "print(network)\n",
    "\n",
    "print(\"=====parameters=====\")\n",
    "for name,weights in network.named_parameters():\n",
    "    print(\"name: \", name)\n",
    "    print(\"weights shape: \", weights.shape)\n",
    "    print(\"weights: \", weights)\n",
    "    print(\"==========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/connectome/ahhyun724/DL_Lab_2024Summer'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNN(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc4): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Ensure the directory exists\n",
    "save_dir = \"./lab1_saves\"\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Option 1: save EVERYTHING (i.e. copy)\n",
    "torch.save(network, os.path.join(save_dir, \"save_option1\"))\n",
    "\n",
    "loaded_network = torch.load(os.path.join(save_dir, \"save_option1\"))\n",
    "print(loaded_network)\n",
    "#print(type(loaded_network))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('fc1.weight', tensor([[-0.0330, -0.0054, -0.0342,  ..., -0.0136, -0.0145,  0.0178],\n",
      "        [ 0.0073, -0.0172, -0.0272,  ..., -0.0058, -0.0112,  0.0325],\n",
      "        [-0.0269,  0.0244, -0.0231,  ...,  0.0094, -0.0296, -0.0269],\n",
      "        ...,\n",
      "        [ 0.0106,  0.0116, -0.0206,  ...,  0.0073,  0.0137, -0.0123],\n",
      "        [-0.0319, -0.0137, -0.0115,  ..., -0.0051, -0.0015,  0.0027],\n",
      "        [ 0.0312,  0.0069,  0.0099,  ...,  0.0145,  0.0003,  0.0097]],\n",
      "       device='cuda:0')), ('fc1.bias', tensor([ 0.0310,  0.0522, -0.1483, -0.0355,  0.0285, -0.0573,  0.1163, -0.1793,\n",
      "        -0.0059, -0.0237,  0.0454, -0.0147, -0.0291, -0.0776,  0.0111,  0.0205,\n",
      "         0.0038,  0.0384, -0.1121,  0.0760, -0.0033, -0.0899,  0.0175, -0.1910,\n",
      "        -0.0660, -0.0766, -0.0457,  0.0113, -0.0182,  0.0126,  0.0262, -0.1074,\n",
      "         0.0830, -0.0178, -0.0841, -0.2541, -0.0003, -0.1276,  0.0768,  0.0388,\n",
      "        -0.0634, -0.0297,  0.0795, -0.0432,  0.0796, -0.0486, -0.0653, -0.0778,\n",
      "         0.0727,  0.1358,  0.0309, -0.0081,  0.0867, -0.0268,  0.0551,  0.0634,\n",
      "         0.0084, -0.0865, -0.0638, -0.0288,  0.0111, -0.0119, -0.1085, -0.0535,\n",
      "         0.0100, -0.0324, -0.0330,  0.0077,  0.0036,  0.1218, -0.0313,  0.0760,\n",
      "        -0.0604,  0.0187,  0.0645, -0.0822,  0.0281, -0.0024, -0.0223,  0.0427,\n",
      "         0.0343,  0.0152,  0.0660,  0.1659, -0.0192, -0.0621,  0.0325, -0.0219,\n",
      "        -0.0497, -0.0551, -0.0782, -0.1017, -0.1735,  0.1067,  0.0467,  0.0988,\n",
      "        -0.0648, -0.0521,  0.0657, -0.0440, -0.0247, -0.2453, -0.0840,  0.0215,\n",
      "        -0.0927,  0.0499, -0.0648, -0.0034,  0.0307,  0.0051, -0.0284, -0.0082,\n",
      "         0.0222,  0.0268, -0.0370,  0.0368,  0.1135, -0.0571,  0.0202, -0.0336,\n",
      "        -0.0546,  0.0136,  0.0210, -0.1435,  0.0060, -0.0458, -0.0304,  0.0124],\n",
      "       device='cuda:0')), ('fc2.weight', tensor([[ 0.0699, -0.1530,  0.1186,  ..., -0.1666,  0.1542,  0.0646],\n",
      "        [ 0.0938,  0.0511,  0.1039,  ...,  0.0061,  0.0592, -0.0076],\n",
      "        [ 0.0436, -0.0220, -0.0122,  ...,  0.0869,  0.0733, -0.0859],\n",
      "        ...,\n",
      "        [ 0.1300, -0.1648, -0.0049,  ..., -0.2016,  0.2896,  0.0362],\n",
      "        [ 0.0369, -0.0069,  0.2003,  ...,  0.0304,  0.1292,  0.0740],\n",
      "        [-0.0908,  0.0299, -0.1856,  ...,  0.2484, -0.1664, -0.0589]],\n",
      "       device='cuda:0')), ('fc2.bias', tensor([-0.0527,  0.0163,  0.0675,  0.1976,  0.0817,  0.1805, -0.0111,  0.1616,\n",
      "         0.0013,  0.0066,  0.1918,  0.1463,  0.0434,  0.0577,  0.0626, -0.0408,\n",
      "        -0.1070,  0.1327,  0.1688,  0.0620,  0.0391, -0.0270,  0.1133, -0.0157,\n",
      "         0.0636, -0.0276,  0.1449, -0.0207,  0.0688,  0.1199, -0.0511,  0.0069,\n",
      "         0.0289, -0.0322, -0.0642,  0.1589,  0.0694, -0.0665,  0.0763, -0.0583,\n",
      "         0.0619, -0.0332, -0.0478, -0.0473, -0.0552, -0.0715, -0.1163, -0.0814,\n",
      "         0.0215, -0.0202, -0.0496, -0.0497,  0.0131,  0.1206, -0.0929, -0.0165,\n",
      "         0.0313, -0.0686, -0.0940,  0.1665,  0.1168,  0.0286,  0.0146,  0.1257],\n",
      "       device='cuda:0')), ('fc3.weight', tensor([[-0.1467,  0.1244,  0.0229,  ...,  0.0044, -0.1939, -0.0456],\n",
      "        [-0.0448,  0.2650, -0.1056,  ...,  0.1370, -0.3713, -0.4108],\n",
      "        [ 0.0201,  0.2276,  0.0368,  ..., -0.0508,  0.0098,  0.2827],\n",
      "        ...,\n",
      "        [-0.2167,  0.0041,  0.1304,  ..., -0.3298, -0.1498,  0.4338],\n",
      "        [ 0.0810, -0.0712,  0.0777,  ...,  0.4210, -0.1426, -0.2544],\n",
      "        [ 0.3099, -0.2245,  0.0949,  ...,  0.1410, -0.1325, -0.0669]],\n",
      "       device='cuda:0')), ('fc3.bias', tensor([ 0.0455,  0.1263, -0.0996,  0.0034, -0.0812,  0.1014, -0.0076, -0.0511,\n",
      "         0.0597,  0.1520,  0.0552,  0.0058,  0.0660,  0.1006, -0.1893,  0.0873,\n",
      "        -0.0620, -0.1334, -0.0826,  0.1020, -0.1674, -0.0529,  0.0418,  0.0317,\n",
      "         0.0425, -0.0327, -0.0075, -0.0166,  0.1408,  0.0762,  0.0789,  0.0243],\n",
      "       device='cuda:0')), ('fc4.weight', tensor([[-1.2384, -0.4354,  0.4775,  0.7206,  0.5117, -0.4341, -1.1096,  1.1938,\n",
      "          0.8528, -0.5689, -0.2095, -0.2306, -0.9451, -0.3992,  1.1545,  0.6006,\n",
      "          1.1306, -0.2959, -0.7608, -0.2664,  0.6091, -0.4507, -0.8968, -0.3013,\n",
      "          0.3946, -0.2926,  1.0082, -0.4256, -1.3059, -1.8735, -0.3335,  0.6844],\n",
      "        [ 0.5433,  0.9458, -0.9600, -0.5445, -0.2385,  0.8177,  0.7829, -0.8220,\n",
      "         -1.4756,  0.1976, -0.5400, -0.0385,  0.6289,  1.2883,  0.7369, -0.4984,\n",
      "         -0.4495,  0.5353,  1.1385,  1.2840, -0.9552, -0.2479,  0.7549,  0.4948,\n",
      "          0.5470, -0.0189, -0.8034, -0.6140,  0.6297,  0.3768, -0.7953, -1.0944],\n",
      "        [-0.2680, -0.6150, -1.3875, -1.0004,  0.3384, -0.5444,  0.7721, -0.0249,\n",
      "         -0.0051, -0.5633,  1.3033, -0.3962,  1.2707, -0.4366,  0.6077,  0.7679,\n",
      "          1.0958,  0.1425,  0.6311, -0.5664,  0.1757, -0.0182,  0.1985,  1.5969,\n",
      "          0.1876,  0.4590, -0.6774,  0.3438, -1.2508, -0.0359, -0.1998,  0.1526],\n",
      "        [ 0.7703, -0.4121,  0.8139, -1.0195, -1.6719,  0.3776,  0.6081,  1.1411,\n",
      "          0.7038,  1.4468, -0.7712, -0.4390,  1.0177, -0.2945,  0.6126, -0.1309,\n",
      "          0.2395, -0.8052,  0.2450, -0.6153,  0.1457, -0.0485, -0.9719,  0.0404,\n",
      "          0.5446, -0.5886, -1.1242,  0.0689,  0.7060,  0.7459, -0.2583, -1.4060],\n",
      "        [-0.9140, -0.1516,  0.0819,  0.6649, -0.0290, -0.5508, -0.3060, -0.5399,\n",
      "         -1.0235, -0.5033,  1.0469,  1.0990, -0.4348, -0.3585, -0.8854,  0.9792,\n",
      "         -0.6689,  0.4719, -0.5522,  0.2916, -0.5770,  1.2499,  0.4436, -0.1235,\n",
      "         -0.9518,  1.2009,  0.2821,  0.7638,  0.4353,  0.3236,  0.9292,  0.0040],\n",
      "        [ 0.7496, -0.4218,  0.2369,  1.8763,  0.7240, -0.4662, -0.3501,  0.0685,\n",
      "          0.7655,  1.7404, -0.4250, -0.1466, -1.1302, -0.2439, -1.1763, -0.0080,\n",
      "         -0.7117, -1.5739,  0.0525, -0.0362,  0.6319, -0.0369, -0.6006, -0.1639,\n",
      "          0.4998, -0.3795,  0.5885, -0.6058,  0.4349,  0.6953, -0.1712, -0.1361],\n",
      "        [-0.8299, -0.6230,  0.3710,  1.1125,  0.5041, -0.7144, -1.1124, -0.8003,\n",
      "         -1.3396, -0.2809,  0.8481, -0.8635, -0.9747, -0.4290,  0.8253,  0.7037,\n",
      "         -0.9253,  0.2644,  1.1062, -0.3637,  0.3144, -0.5764,  0.6968, -0.8137,\n",
      "          0.6949,  1.1971,  0.5477, -0.8914, -1.1534,  0.7878, -1.0999,  0.2684],\n",
      "        [ 0.2862,  0.8298, -1.6059, -0.1912, -0.3288,  0.6298,  0.5761,  1.0426,\n",
      "          0.4881, -0.4763,  0.0899,  0.4062,  0.6884,  1.7086, -0.2944, -0.1478,\n",
      "          0.6434,  0.5194, -0.5196,  0.2875, -0.9247,  0.0680, -1.0166,  0.6787,\n",
      "         -0.9938, -0.3417,  0.1003,  0.6820,  0.4853, -1.3419,  0.9767,  0.2505],\n",
      "        [ 0.6295, -0.7480,  0.6904, -1.1241,  1.1576,  0.1538,  1.2679, -0.9861,\n",
      "          0.4637, -1.1760, -0.6277, -0.5850,  0.2219, -0.6575, -0.7506, -1.0607,\n",
      "          0.2717,  0.4695, -0.3953, -0.4371,  0.3812, -0.7179,  1.1707, -0.9587,\n",
      "          0.5123, -0.5874, -0.9263, -0.6857,  0.9653,  0.4691, -0.3992,  1.1457],\n",
      "        [ 0.1616,  1.1009,  0.9357, -0.9305, -0.7437,  1.3158, -1.0392,  0.0086,\n",
      "          0.6851,  0.1216, -0.8501,  0.7945, -0.3714, -0.7694, -0.7447, -1.2795,\n",
      "         -0.8444,  0.4980, -0.7487,  0.0789,  0.2119,  1.1515,  0.1625, -0.4898,\n",
      "         -0.5664, -0.8355,  0.8560,  0.9204,  0.2701,  0.1533,  0.9255,  0.1135]],\n",
      "       device='cuda:0')), ('fc4.bias', tensor([ 0.2286, -0.0657, -0.2690, -0.0322, -0.2262,  0.1610, -0.2609, -0.0872,\n",
      "        -0.1328,  0.0542], device='cuda:0'))])\n"
     ]
    }
   ],
   "source": [
    "print(network.state_dict())\n",
    "torch.save(network.state_dict(), os.path.join(save_dir, \"save_option2\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3D_CNN_woPip_kernel",
   "language": "python",
   "name": "3d_cnn_wopip_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
